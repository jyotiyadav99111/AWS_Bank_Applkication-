{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2627c233",
   "metadata": {},
   "source": [
    "# AWS Demonstration \n",
    "\n",
    "\n",
    "The code defines the way in which any model can be deployed over the AWS for production purpose. Each and every instruction has been proided in the sheet. \n",
    "\n",
    "Please follow the following steps to create the instance in AWS:\n",
    "1. Create AWS Account (No money required)\n",
    "2. Login into AWS Management console\n",
    "3. Search for AWS Sagemaker\n",
    "4. Left hand side panel navigation --> Notebook > Notebook Instances > Create Notebook Instances\n",
    "5. Create IAM Role while creating teh notebook instance (This is essential part of the process because it helps manage the access of the notebook)\n",
    "6. The instance creation will take some time. Once done, please open it and start writing the below code\n",
    "\n",
    "The demonstration is for the XGBoost model\n",
    "\n",
    "The process goes as follows:\n",
    "1. Set up environment\n",
    "2. Download and split dataset\n",
    "3. Model\n",
    "4. Deployment \n",
    "5. Predictions\n",
    "6. Delete endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ab855",
   "metadata": {},
   "source": [
    "## Environment & path set up\n",
    "\n",
    "There are some libraries which are required to be installed before we begin with the entire process. \n",
    "1. sagemaker -- inbult engine to perfome modeling and deployment\n",
    "2. boto3 -- help in connecting server with this machine instance\n",
    "\n",
    "Specific functions import:\n",
    "1. get_image_uri -- As AWS inbuilt model will be used, it is required to be fetched as a container through this fuction\n",
    "2. csv_serializer -- This will be used for prediction purpose, as the input will be supplied in the form of the csv (serialisation of the input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a5b7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker \n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri \n",
    "from sagemaker.session import s3_input, Session\n",
    "from sagemaker.predictor import csv_serializer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed90ec",
   "metadata": {},
   "source": [
    "This step is to automatically create the S3 bucket in the AWS\n",
    "1. specify the bucket name as per availablility\n",
    "2. fetch the region-name (Note: S3 buckets are free of region specification)\n",
    "\n",
    "(Note: S3 (Simple storage service) bucket is a data storage platform which provides scalability as well.\n",
    "Region name: location of the operation performed (Eg: N. Vergnia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1332c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'ba-data-112233'\n",
    "#get region name\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106d8fb",
   "metadata": {},
   "source": [
    "The below code is to connect to S3 (using boto3) and create bucket for the project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1ca0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "#get access of the s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "#create bucket\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2dbb72",
   "metadata": {},
   "source": [
    "The bucket will be used for storing everthing produced by the entire model. It involves:\n",
    "1. Original data file\n",
    "2. train and test data (created by spliting function)\n",
    "3. model file\n",
    "4. predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76d9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://ba-data-112233/xgboost_model/output\n"
     ]
    }
   ],
   "source": [
    "# set an output path where the trained model will be saved\n",
    "prefix = 'xgboost_model'\n",
    "output_path ='s3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d8210",
   "metadata": {},
   "source": [
    "## Data Download and spliting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9bdf9",
   "metadata": {},
   "source": [
    "The data is downlaoded from the github page through urllib library. and then finally converted to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b987557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Download data in s3 bucket\n",
    "try:\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/jyotiyadav99111/AWS_Bank_Applkication-/main/bank_data.csv', 'bank_data.csv')\n",
    "    print('Data Downloaded successfully')\n",
    "except Exception as e:\n",
    "    print('Downloading error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf26e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datafame created successfully\n"
     ]
    }
   ],
   "source": [
    "# load dataset in pandas dataframe\n",
    "try:\n",
    "    df = pd.read_csv('./bank_data.csv')   # provide relative path in s3 bucket\n",
    "    print('Datafame created successfully')\n",
    "except Exception as e:\n",
    "    print('Dataframe creation error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "273c417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 62) (8238, 62)\n"
     ]
    }
   ],
   "source": [
    "# Train and test data split\n",
    "train_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.8 * len(df))])\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cdaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per some documentations in AWS the target variable should be the first column\n",
    "# training data saved as csv\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "# upload data to s3 bucket under the 'train' folder\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "# for general upload of the we will reuire path of the data next time \n",
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0d07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same for test data\n",
    "\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.TrainingInput(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361fd8b",
   "metadata": {},
   "source": [
    "## XGBoost Model \n",
    "\n",
    "The AWS inbuilt models are built in form of containers or images. These are required to be pulled off and loaded in the instance for the use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e563479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# any algo can be called using this method (not necessarily the xgboost)\n",
    "xgboost_container = get_image_uri(boto3.Session().region_name,'xgboost', repo_version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dea5071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These hyperparameter have been tuned already on local machine as on AWS it will be alittle slow and costly\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b689784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_max_run has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_use_spot_instances has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_max_wait has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# This is a gernal method can be used for any ML algorithms, you just need to specify it in the container itslef\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m5.2xlarge', \n",
    "                                          train_volume_size=5, # 5 GB \n",
    "                                          output_path=output_path,\n",
    "                                          train_use_spot_instances=True,\n",
    "                                          train_max_run=300,\n",
    "                                          train_max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3209ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-24 12:07:55 Starting - Starting the training job...\n",
      "2021-07-24 12:08:20 Starting - Launching requested ML instancesProfilerReport-1627128475: InProgress\n",
      "......\n",
      "2021-07-24 12:09:20 Starting - Preparing the instances for training......\n",
      "2021-07-24 12:10:20 Downloading - Downloading input data\n",
      "2021-07-24 12:10:20 Training - Downloading the training image...\n",
      "2021-07-24 12:10:53 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-07-24:12:10:48:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-07-24:12:10:48:INFO] File size need to be processed in the node: 5.05mb. Available memory size in the node: 23804.71mb\u001b[0m\n",
      "\u001b[34m[2021-07-24:12:10:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[12:10:48] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[12:10:48] 32950x60 matrix with 1977000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2021-07-24:12:10:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[12:10:48] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[12:10:48] 8238x60 matrix with 494280 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.096965#011validation-error:0.105123\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.095751#011validation-error:0.103302\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.095903#011validation-error:0.102331\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.095781#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.095751#011validation-error:0.102816\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.095478#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.095205#011validation-error:0.103302\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.095144#011validation-error:0.102331\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.094901#011validation-error:0.102209\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.094841#011validation-error:0.102088\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.095114#011validation-error:0.101724\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.094689#011validation-error:0.102331\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.094537#011validation-error:0.102088\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.094143#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.094173#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.093991#011validation-error:0.101967\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.093657#011validation-error:0.101724\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.093718#011validation-error:0.101845\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.093657#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.093384#011validation-error:0.103423\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.093505#011validation-error:0.103909\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.093687#011validation-error:0.10403\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.093505#011validation-error:0.103666\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.093536#011validation-error:0.103787\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.093445#011validation-error:0.103666\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.093202#011validation-error:0.102816\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.092473#011validation-error:0.103059\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.092443#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.092534#011validation-error:0.102209\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.092747#011validation-error:0.102938\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.092625#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.092534#011validation-error:0.102695\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.092473#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.092382#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.09217#011validation-error:0.102331\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.0922#011validation-error:0.102695\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.092049#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.091684#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.091745#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.091654#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.091684#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.091624#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.091411#011validation-error:0.102695\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.091411#011validation-error:0.102816\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.09132#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.091381#011validation-error:0.102816\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.091381#011validation-error:0.102938\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.091381#011validation-error:0.102938\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.091199#011validation-error:0.102938\u001b[0m\n",
      "\u001b[34m[12:10:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.091108#011validation-error:0.102452\u001b[0m\n",
      "\n",
      "2021-07-24 12:11:20 Completed - Training job completed\n",
      "Training seconds: 50\n",
      "Billable seconds: 23\n",
      "Managed Spot Training savings: 54.0%\n"
     ]
    }
   ],
   "source": [
    "# final training of the model given the chosen paramters\n",
    "estimator.fit({'train': s3_input_train,'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e78d0",
   "metadata": {},
   "source": [
    "## Deploy the model as endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4fb154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80efa8",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26757cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8238,)\n"
     ]
    }
   ],
   "source": [
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "xgb_predictor.serializer = csv_serializer # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86f7a0",
   "metadata": {},
   "source": [
    "The data is clearly unbalanced but the mail focus is on the AWS demonstration. Below code has been taken from the AWS documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e1c58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.8%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    91% (7132)    34% (138)\n",
      "Purchase        9% (706)     66% (262) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e128ee",
   "metadata": {},
   "source": [
    "## Delete the endpoint and all the corresponding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de420d4",
   "metadata": {},
   "source": [
    "Once the model has been created and everything has been acomplished it is a good idea to delete the data and other files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ba69414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': 'H3EKGCJE9Q5DAZXC',\n",
       "   'HostId': '996z3I7Gk0t8+SoWa2hb0mjB/xSODxA4dbY9F9MJRGWgOlWq90aKtUfXtKc3+mgnLfb44KHw9xk=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': '996z3I7Gk0t8+SoWa2hb0mjB/xSODxA4dbY9F9MJRGWgOlWq90aKtUfXtKc3+mgnLfb44KHw9xk=',\n",
       "    'x-amz-request-id': 'H3EKGCJE9Q5DAZXC',\n",
       "    'date': 'Sat, 24 Jul 2021 12:38:48 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'xgboost_model/output/xgboost-2021-07-24-12-07-55-586/profiler-output/framework/training_job_end.ts'},\n",
       "   {'Key': 'xgboost_model/output/xgboost-2021-07-24-12-07-55-586/profiler-output/system/incremental/2021072412/1627128600.algo-1.json'},\n",
       "   {'Key': 'xgboost_model/test/test.csv'},\n",
       "   {'Key': 'xgboost_model/train/train.csv'},\n",
       "   {'Key': 'xgboost_model/output/xgboost-2021-07-24-12-07-55-586/output/model.tar.gz'},\n",
       "   {'Key': 'xgboost_model/output/xgboost-2021-07-24-12-07-55-586/profiler-output/system/training_job_end.ts'}]}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
